{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227b35f7",
   "metadata": {},
   "source": [
    "# 00 — Read CSV files (of CIC-IDS2017)\n",
    "\n",
    "This notebook reads the original CIC-IDS2017 CSV files in a **memory‑friendly, chunked** manner, applies **lightweight cleaning**, **normalizes odd dash characters in labels**, and writes **one single combined Parquet** file.\n",
    "\n",
    "### What this notebook includes\n",
    "- **Column name standardization** (spaces/specials → `_`, trimmed, de‑duplicated).\n",
    "- Replacement of problematic values (`±Infinity` → `NaN`; also recognizes literal `\"Infinity\"` tokens during CSV parsing).\n",
    "- String columns stripped of leading/trailing whitespace.\n",
    "- **Robust normalization of “dash-like” characters in `Label`**, e.g.  \n",
    "  `Web Attack � Brute Force`, `Web Attack – XSS`, `Web Attack — Sql Injection`  \n",
    "  ⟶ **`Web Attack - Brute Force`**, **`Web Attack - XSS`**, **`Web Attack - Sql Injection`**.\n",
    "- **Streamed** combination into `data/processed/cicids2017_full_clean.parquet`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb1d07",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb78696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\cedric\\code\\VS\\Studium\\Hausarbeit\\Code\n",
      "DATA_DIR: C:\\Users\\cedric\\code\\VS\\Studium\\Hausarbeit\\Code\\data\\raw\\MachineLearningCSV\n",
      "OUTPUT_DIR: C:\\Users\\cedric\\code\\VS\\Studium\\Hausarbeit\\Code\\data\\processed\n",
      "COMBINED_PARQUET_PATH: C:\\Users\\cedric\\code\\VS\\Studium\\Hausarbeit\\Code\\data\\processed\\cicids2017_full_clean.parquet\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "from pathlib import Path\n",
    "\n",
    "# Project root (assumes this notebook is in a 'notebooks/' dir)\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "# Input: original CIC-IDS2017 CSVs (non-recursive)\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\" / \"MachineLearningCSV\"\n",
    "\n",
    "# Output: combined cleaned dataset\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Where to write the final combined, cleaned Parquet file\n",
    "COMBINED_PARQUET_PATH = OUTPUT_DIR / \"cicids2017_full_clean.parquet\"\n",
    "\n",
    "# Parquet compression\n",
    "PARQUET_COMPRESSION = \"snappy\"\n",
    "\n",
    "# Chunk size for streaming CSV reads\n",
    "CHUNKSIZE = 100_000\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "print(\"COMBINED_PARQUET_PATH:\", COMBINED_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb6fee",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "The utilities below do four key things:\n",
    "\n",
    "1. **List** raw CSV files to process.  \n",
    "2. **Standardize** column names to avoid downstream issues (e.g., spaces, `/`, `-`, `.`) and ensure uniqueness.  \n",
    "3. **Clean** each chunk:\n",
    "   - strip leading/trailing whitespace from strings,\n",
    "   - replace `±Infinity` with `NaN`,\n",
    "   - **normalize dash-like characters in `Label`** into a plain ASCII hyphen with spaces (`\" - \"`).  \n",
    "4. **Stream** the cleaned chunks into **one** Parquet file to keep memory usage bounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1545e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def list_raw_csvs(data_dir: Path, pattern: str = \"*.csv\") -> List[Path]:\n",
    "    \"\"\"Return a sorted list of CSV files in the given directory (non-recursive).\"\"\"\n",
    "    files = sorted(data_dir.glob(pattern))\n",
    "    if not files:\n",
    "        print(f\"[WARN] No CSV files found in {data_dir}\")\n",
    "    return files\n",
    "\n",
    "def standardize_column_names(cols: Iterable[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Normalize column names:\n",
    "      - strip leading/trailing whitespace,\n",
    "      - collapse inner spaces,\n",
    "      - replace common specials with '_',\n",
    "      - normalize 'Label' capitalization,\n",
    "      - ensure uniqueness after normalization by adding suffixes '__1', '__2', ...\n",
    "    \"\"\"\n",
    "    normalized: List[str] = []\n",
    "    for c in cols:\n",
    "        c0 = str(c).strip()\n",
    "        c0 = \" \".join(c0.split())  # collapse multiple spaces\n",
    "        for ch in [' ', '/', '(', ')', '-', ':', ',', '\\\\', '.']:\n",
    "            c0 = c0.replace(ch, '_')\n",
    "        while \"__\" in c0:\n",
    "            c0 = c0.replace(\"__\", \"_\")\n",
    "        if c0.lower() == \"label\":\n",
    "            c0 = \"Label\"\n",
    "        normalized.append(c0)\n",
    "\n",
    "    # Deduplicate after normalization\n",
    "    seen: Dict[str, int] = {}\n",
    "    result: List[str] = []\n",
    "    for name in normalized:\n",
    "        if name not in seen:\n",
    "            seen[name] = 0\n",
    "            result.append(name)\n",
    "        else:\n",
    "            seen[name] += 1\n",
    "            result.append(f\"{name}__{seen[name]}\")\n",
    "    return result\n",
    "\n",
    "# --- Dash / replacement-character normalization for labels --------------------\n",
    "# Treat hyphen-minus and many dash-like Unicode chars + the replacement char U+FFFD as one class\n",
    "_DASH_CLASS = r\"[-\\u2010-\\u2015\\u2212\\u2043\\uFE58\\uFE63\\uFF0D\\uFFFD]\"\n",
    "\n",
    "def normalize_label_dashes(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Unify various 'dash-like' characters to a plain ' - ' pattern and tidy spaces.\"\"\"\n",
    "    return (\n",
    "        series.astype(\"string\")\n",
    "        .str.normalize(\"NFKC\")                                  # Unicode normalize\n",
    "        .str.replace(fr\"\\s*{_DASH_CLASS}\\s*\", \" - \", regex=True)  # unify to ' - '\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)                   # collapse spaces\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "def clean_chunk(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply minimal cleaning on a chunk:\n",
    "      - Rename columns consistently,\n",
    "      - Strip strings in object columns,\n",
    "      - Replace ±Infinity with NaN,\n",
    "      - Normalize dash characters inside 'Label' so names are consistent.\n",
    "    Note: we keep 'Label' as string to avoid category schema friction across chunks.\n",
    "    \"\"\"\n",
    "    df.columns = standardize_column_names(df.columns)\n",
    "\n",
    "    # Strip textual columns\n",
    "    for c in df.select_dtypes(include=['object']).columns:\n",
    "        df[c] = df[c].astype('string').str.strip()\n",
    "\n",
    "    # Replace infinities\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Normalize 'Label' dashes if present\n",
    "    if 'Label' in df.columns:\n",
    "        df['Label'] = normalize_label_dashes(df['Label'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def iter_csv_in_chunks(path: Path, chunksize: int = 100_000):\n",
    "    \"\"\"\n",
    "    Yield CLEANED chunks from a CSV (streaming for low memory usage).\n",
    "    Also treats literal 'Infinity' tokens as NaN.\n",
    "    \"\"\"\n",
    "    for chunk in pd.read_csv(\n",
    "        path,\n",
    "        low_memory=False,\n",
    "        chunksize=chunksize,\n",
    "        na_values=['Infinity', 'inf', 'NaN', 'nan']\n",
    "    ):\n",
    "        yield clean_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71164a79",
   "metadata": {},
   "source": [
    "## Writer: combine into one cleaned Parquet\n",
    "\n",
    "This writes the merged, cleaned dataset in a **streaming** fashion so you can process large files on modest hardware. We rely on `pyarrow`'s `ParquetWriter` to append **row groups** as we go.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20cae957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_to_parquet_cleaned(\n",
    "    files: List[Path],\n",
    "    out_path: Path,\n",
    "    compression: str = \"snappy\",\n",
    "    chunksize: int = 100_000,\n",
    ") -> Path:\n",
    "    \"\"\"Combine all CSVs into ONE cleaned Parquet (streamed).\"\"\"\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    writer = None\n",
    "    total_rows = 0\n",
    "    for f in files:\n",
    "        print(f\"[INFO] Processing: {f.name}\")\n",
    "        for chunk in iter_csv_in_chunks(f, chunksize=chunksize):\n",
    "            total_rows += len(chunk)\n",
    "            table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(out_path, table.schema, compression=compression)\n",
    "            writer.write_table(table)\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    print(f\"[DONE] Wrote {total_rows} cleaned rows to -> {out_path}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d0c016",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "- **Lists** found CSVs.\n",
    "- **Streams** them into the combined cleaned Parquet.\n",
    "- Prints a quick metadata check (row count / row groups) without loading everything into RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b82b5a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV files: 8\n",
      " - Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      " - Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      " - Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      " - Monday-WorkingHours.pcap_ISCX.csv\n",
      " - Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      " - Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      " - Tuesday-WorkingHours.pcap_ISCX.csv\n",
      " - Wednesday-workingHours.pcap_ISCX.csv\n",
      "[INFO] Processing: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "[INFO] Processing: Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "[INFO] Processing: Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "[INFO] Processing: Monday-WorkingHours.pcap_ISCX.csv\n",
      "[INFO] Processing: Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "[INFO] Processing: Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "[INFO] Processing: Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "[INFO] Processing: Wednesday-workingHours.pcap_ISCX.csv\n",
      "[DONE] Wrote 2830743 cleaned rows to -> C:\\Users\\cedric\\code\\VS\\Studium\\Hausarbeit\\Code\\data\\processed\\cicids2017_full_clean.parquet\n",
      "Parquet metadata: 2830743 rows, 31 row groups.\n"
     ]
    }
   ],
   "source": [
    "files = list_raw_csvs(DATA_DIR, pattern=\"*.csv\")\n",
    "print(\"Found CSV files:\", len(files))\n",
    "for p in files:\n",
    "    print(\" -\", p.name)\n",
    "\n",
    "if not files:\n",
    "    raise SystemExit(\"No CSVs found. Please check DATA_DIR.\")\n",
    "\n",
    "# Build the combined Parquet (cleaned)\n",
    "combine_to_parquet_cleaned(\n",
    "    files=files,\n",
    "    out_path=COMBINED_PARQUET_PATH,\n",
    "    compression=PARQUET_COMPRESSION,\n",
    "    chunksize=CHUNKSIZE,\n",
    ")\n",
    "\n",
    "# Lightweight validation: read row count from Parquet metadata (no full load)\n",
    "try:\n",
    "    import pyarrow.parquet as pq\n",
    "    pf = pq.ParquetFile(COMBINED_PARQUET_PATH)\n",
    "    print(f\"Parquet metadata: {pf.metadata.num_rows} rows, {pf.metadata.num_row_groups} row groups.\")\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Could not read Parquet metadata:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
