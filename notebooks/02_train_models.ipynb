{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc67dbd-328d-4b33-89ab-0527ed83629a",
   "metadata": {},
   "source": [
    "# 02 — Train Models (RF, IF, ET-SSL)\n",
    "\n",
    "This notebook trains the three selected approaches under **two scenarios**:\n",
    "\n",
    "- **Base** — use all classes normally.  \n",
    "- **Zero-Day** — exclude `Bot`, `Web Attack - Brute Force`, and `Infiltration` from train/val; they only reappear in test.  \n",
    "\n",
    "### Implemented approaches\n",
    "- **Random Forest (RF, supervised)** — with randomized hyperparameter search, class-weight balancing, and threshold tuning.  \n",
    "- **Isolation Forest (IF, unsupervised)** — trained on BENIGN only, small grid over core hyperparameters.  \n",
    "- **ET-SSL (self-supervised)** — contrastive encoder with augmentations, batch norm, dropout, and anomaly-scoring setup.  \n",
    "\n",
    "### Notes\n",
    "- All runs are **seeded** for reproducibility.  \n",
    "- RF/IF: implemented via scikit-learn pipelines (including PCA grouping transformer).  \n",
    "- ET-SSL: trained in PyTorch, GPU-accelerated if available.  \n",
    "- Model-specific metadata (e.g., thresholds, centroids, scaling stats) are saved for later evaluation.  \n",
    "\n",
    "**Inputs**: `data/splits/*` from notebook 01  \n",
    "**Outputs**:  \n",
    "- RF/IF: `models/*.joblib` + `*_meta.json`  \n",
    "- ET-SSL: `models/etssl_*_encoder.pt` + `etssl_*_meta.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05584bc-0024-494a-b832-544bdbc681b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Imports & configuration\n",
    "\n",
    "import os, json, time, math, warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Any, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, ParameterSampler\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "SPLITS_DIR = ROOT / \"data\" / \"splits\"\n",
    "MODELS_DIR = ROOT / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LABEL_COL = \"label\"\n",
    "BENIGN_NAME = \"BENIGN\"\n",
    "USE_CG_PCA = True  # toggle\n",
    "CV_SUBSAMPLE_CAP = 150_000   # subsample for RF CV if train is huge\n",
    "FINAL_RF_TREES = 800         # warm-start grows to this many trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c320c-a910-4d4f-a542-920a35c33f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Utilities: split loader, threshold search, CorrelatedGroupsPCA\n",
    "\n",
    "def load_split(approach: str, scenario: str, split: str) -> pd.DataFrame:\n",
    "    p = SPLITS_DIR / approach / scenario / f\"{split}.parquet\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "    return pd.read_parquet(p)\n",
    "\n",
    "def features_and_labels(df: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    X = df.drop(columns=[LABEL_COL, \"Label\"], errors=\"ignore\")\n",
    "    y = df[LABEL_COL].values.astype(np.int64)\n",
    "    return X, y\n",
    "\n",
    "def best_threshold_from_scores(y_true: np.ndarray, scores: np.ndarray, maximize: str = \"f1_macro\") -> Tuple[float, float]:\n",
    "    \"\"\"Find threshold that maximizes macro-F1 by scanning unique score quantiles.\"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import f1_score\n",
    "    y_true = y_true.astype(int)\n",
    "    # quantile grid (fast, robust)\n",
    "    qs = np.linspace(0.01, 0.99, 99)\n",
    "    thr_grid = np.quantile(scores, qs)\n",
    "    best = (-1.0, 0.5)\n",
    "    for thr in np.unique(thr_grid):\n",
    "        yhat = (scores >= thr).astype(int)\n",
    "        f1m = f1_score(y_true, yhat, average=\"macro\", zero_division=0)\n",
    "        if f1m > best[0]:\n",
    "            best = (f1m, thr)\n",
    "    return float(best[1]), float(best[0])\n",
    "\n",
    "class CorrelatedGroupsPCA:\n",
    "    \"\"\"Lightweight transformer: groups |corr|>=rho and applies PCA per group to keep var_keep.\"\"\"\n",
    "    def __init__(self, rho: float = 0.95, var_keep: float = 0.99):\n",
    "        self.rho = float(rho)\n",
    "        self.var_keep = float(var_keep)\n",
    "        self.groups_ = None\n",
    "        self.columns_ = None\n",
    "        self.pca_models_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from sklearn.decomposition import PCA\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X_df = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n",
    "        else:\n",
    "            X_df = X.copy()\n",
    "        self.columns_ = list(X_df.columns)\n",
    "        corr = X_df.corr(numeric_only=True).fillna(0.0).values\n",
    "        n = corr.shape[0]\n",
    "        visited = np.zeros(n, dtype=bool)\n",
    "        groups = []\n",
    "        for i in range(n):\n",
    "            if visited[i]: continue\n",
    "            g = [i]\n",
    "            visited[i] = True\n",
    "            for j in range(i+1, n):\n",
    "                if visited[j]: continue\n",
    "                if abs(corr[i, j]) >= self.rho:\n",
    "                    g.append(j); visited[j] = True\n",
    "            groups.append(sorted(g))\n",
    "        # Build models per group\n",
    "        self.pca_models_ = []\n",
    "        for g in groups:\n",
    "            if len(g) == 1:\n",
    "                self.pca_models_.append((\"pass\", g, None))\n",
    "            else:\n",
    "                pca = PCA(n_components=None, svd_solver=\"full\", random_state=SEED)\n",
    "                pca.fit(X_df.iloc[:, g])\n",
    "                # select number of components to reach var_keep\n",
    "                csum = np.cumsum(pca.explained_variance_ratio_)\n",
    "                k = int(np.searchsorted(csum, self.var_keep) + 1)\n",
    "                k = max(1, min(k, len(g)))\n",
    "                pca_k = PCA(n_components=k, svd_solver=\"full\", random_state=SEED).fit(X_df.iloc[:, g])\n",
    "                self.pca_models_.append((\"pca\", g, pca_k))\n",
    "        self.groups_ = groups\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X_df = pd.DataFrame(X, columns=self.columns_)\n",
    "        else:\n",
    "            X_df = X.copy()\n",
    "        outs = []\n",
    "        for kind, g, model in self.pca_models_:\n",
    "            if kind == \"pass\":\n",
    "                outs.append(X_df.iloc[:, g].values)\n",
    "            else:\n",
    "                outs.append(model.transform(X_df.iloc[:, g]))\n",
    "        return np.concatenate(outs, axis=1)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142306f5-1249-4787-ba5d-db8b4a4070c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Random Forest — with CV progress & warm-start final fit\n",
    "\n",
    "from joblib import parallel_backend\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, ParameterSampler\n",
    "\n",
    "def _rf_pipe_for_cv():\n",
    "    return Pipeline([\n",
    "        (\"var0\", VarianceThreshold(0.0)),\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        (\"cgpca\", CorrelatedGroupsPCA(0.95, 0.99) if USE_CG_PCA else \"passthrough\"),\n",
    "        (\"rf\", RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "            max_depth=None, max_features=\"sqrt\", min_samples_leaf=1,\n",
    "            n_jobs=-1, random_state=SEED\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def train_rf(scenario: str) -> Dict[str, Any]:\n",
    "    # Load splits\n",
    "    tr = load_split(\"rf\", scenario, \"train\")\n",
    "    va = load_split(\"rf\", scenario, \"val\")\n",
    "    X_tr, y_tr = features_and_labels(tr)\n",
    "    X_va, y_va = features_and_labels(va)\n",
    "\n",
    "    # Stratified subsample for CV\n",
    "    if len(X_tr) > CV_SUBSAMPLE_CAP:\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, train_size=CV_SUBSAMPLE_CAP, random_state=SEED)\n",
    "        idx, _ = next(sss.split(X_tr, y_tr))\n",
    "        X_cv, y_cv = X_tr.iloc[idx].copy(), y_tr[idx].copy()\n",
    "    else:\n",
    "        X_cv, y_cv = X_tr, y_tr\n",
    "\n",
    "    # Param space\n",
    "    param_space = {\n",
    "        \"rf__max_depth\": [10, 20, 30, None],\n",
    "        \"rf__max_features\": [\"sqrt\", 0.6, 0.8],\n",
    "        \"rf__min_samples_leaf\": [1, 2, 4],\n",
    "    }\n",
    "    n_iter = 12\n",
    "    sampler = list(ParameterSampler(param_space, n_iter=n_iter, random_state=SEED))\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "    total_fits = n_iter * cv.get_n_splits()\n",
    "\n",
    "    # Manual CV (live progress)\n",
    "    pipe_template = _rf_pipe_for_cv()\n",
    "    fit_times: List[float] = []\n",
    "    results: List[Tuple[dict, float]] = []\n",
    "\n",
    "    print(f\"[RF/{scenario}] CV: {n_iter} candidates × {cv.get_n_splits()} folds = {total_fits} fits on {len(X_cv)} rows\")\n",
    "    t_search0 = time.time()\n",
    "    with parallel_backend(\"threading\"):\n",
    "        pbar = tqdm(total=total_fits, desc=f\"RF {scenario} CV\", unit=\"fit\")\n",
    "        for params in sampler:\n",
    "            scores = []\n",
    "            for tr_idx, va_idx in cv.split(X_cv, y_cv):\n",
    "                Xtr, Xv = X_cv.iloc[tr_idx], X_cv.iloc[va_idx]\n",
    "                ytr, yv = y_cv[tr_idx], y_cv[va_idx]\n",
    "                model = Pipeline(pipe_template.steps)  # fresh clone\n",
    "                model.set_params(**params)\n",
    "                t0 = time.time()\n",
    "                model.fit(Xtr, ytr)\n",
    "                prob = model.predict_proba(Xv)[:, 1]\n",
    "                yhat = (prob >= 0.5).astype(int)\n",
    "                f1m = f1_score(yv, yhat, average=\"macro\", zero_division=0)\n",
    "                dt = time.time() - t0\n",
    "                fit_times.append(dt); scores.append(f1m)\n",
    "\n",
    "                done = len(fit_times)\n",
    "                avg = np.mean(fit_times)\n",
    "                eta = max(0, (total_fits - done) * avg)\n",
    "                pbar.set_postfix({\"avg_s\": f\"{avg:.1f}\", \"ETA\": time.strftime(\"%H:%M:%S\", time.gmtime(eta))})\n",
    "                pbar.update(1)\n",
    "            results.append((params, float(np.mean(scores))))\n",
    "        pbar.close()\n",
    "    search_time = time.time() - t_search0\n",
    "\n",
    "    results.sort(key=lambda kv: kv[1], reverse=True)\n",
    "    best_params, best_cv_f1 = results[0]\n",
    "    print(f\"[RF/{scenario}] best CV macro-F1={best_cv_f1:.4f} params={best_params}\")\n",
    "\n",
    "    # Final fit on full train with warm-start & ETA (no resampling)\n",
    "    prep = Pipeline([\n",
    "        (\"var0\", VarianceThreshold(0.0)),\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        (\"cgpca\", CorrelatedGroupsPCA(0.95, 0.99) if USE_CG_PCA else \"passthrough\"),\n",
    "    ])\n",
    "    Xtr_tf = prep.fit_transform(X_tr, y_tr)\n",
    "    Xva_tf = prep.transform(X_va)\n",
    "\n",
    "    # If CV found max_depth=None, cap for runtime safety\n",
    "    max_depth_final = best_params.get(\"rf__max_depth\", None)\n",
    "    if max_depth_final is None:\n",
    "        max_depth_final = 30\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=0, warm_start=True,  # incremental\n",
    "        max_depth=max_depth_final,\n",
    "        max_features=best_params.get(\"rf__max_features\", \"sqrt\"),\n",
    "        min_samples_leaf=best_params.get(\"rf__min_samples_leaf\", 1),\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1, random_state=SEED\n",
    "    )\n",
    "\n",
    "    n_total = FINAL_RF_TREES\n",
    "    step = 100\n",
    "    pbar = tqdm(total=n_total, desc=f\"RF {scenario} final trees\", unit=\"tree\")\n",
    "    times = []; built = 0\n",
    "    t_fit0 = time.time()\n",
    "    while built < n_total:\n",
    "        target = min(n_total, built + step)\n",
    "        rf.set_params(n_estimators=target)\n",
    "        t0 = time.time()\n",
    "        rf.fit(Xtr_tf, y_tr)\n",
    "        dt = time.time() - t0\n",
    "        times.append(dt)\n",
    "        built = target\n",
    "        avg_per_tree = (np.sum(times) / built) if built else 0.0\n",
    "        eta = (n_total - built) * avg_per_tree\n",
    "        pbar.set_postfix({\"avg_s/tree\": f\"{avg_per_tree:.2f}\", \"ETA\": time.strftime(\"%H:%M:%S\", time.gmtime(eta))})\n",
    "        pbar.update(step)\n",
    "    pbar.close()\n",
    "    final_time = time.time() - t_fit0\n",
    "\n",
    "    # Threshold on validation\n",
    "    val_proba = rf.predict_proba(Xva_tf)[:, 1]\n",
    "    thr, f1m = best_threshold_from_scores(y_va, val_proba)\n",
    "\n",
    "    # Wrap into a single pipeline for saving\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    final_pipe = make_pipeline(prep, rf)\n",
    "\n",
    "    model_path = MODELS_DIR / f\"rf_{scenario}.joblib\"\n",
    "    joblib.dump(final_pipe, model_path)\n",
    "    meta = {\n",
    "        \"scenario\": scenario,\n",
    "        \"threshold\": float(thr),\n",
    "        \"val_macro_f1_at_threshold\": float(f1m),\n",
    "        \"best_params\": {**best_params,\n",
    "                        \"final_n_estimators\": int(n_total),\n",
    "                        \"max_depth_final\": int(max_depth_final) if max_depth_final is not None else None,\n",
    "                        \"class_weight\": \"balanced_subsample\"},\n",
    "        \"cv_subsample_rows\": int(len(X_cv)),\n",
    "        \"search_time_sec\": float(search_time),\n",
    "        \"final_train_time_sec\": float(final_time),\n",
    "        \"use_cg_pca\": bool(USE_CG_PCA)\n",
    "    }\n",
    "    with open(MODELS_DIR / f\"rf_{scenario}_meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    return {\"model_path\": str(model_path), **meta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174533b0-2c0c-4bcf-8047-c6a18edec5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Isolation Forest — stronger search (contamination, seeds, scalers) with ETA\n",
    "\n",
    "def train_if(scenario: str, mode: str = \"speed\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    mode: \"speed\" (sehr schnell, guter Start) oder \"balanced\" (etwas breiter, dennoch flott)\n",
    "    \"\"\"\n",
    "    assert mode in {\"speed\", \"balanced\"}\n",
    "    tr = load_split(\"if\", scenario, \"train\")   # benign-only\n",
    "    va = load_split(\"if\", scenario, \"val\")\n",
    "    X_tr_df, _     = features_and_labels(tr)\n",
    "    X_va_df, y_va  = features_and_labels(va)\n",
    "\n",
    "    if mode == \"speed\":\n",
    "        PREPROCS = [(\"minmax\", MinMaxScaler())]\n",
    "        grid_ne   = [200, 400]\n",
    "        grid_ms   = [\"auto\", 256]        # \"auto\" ~ min(256, n_samples)\n",
    "        grid_cont = [0.002, 0.005, 0.01] # CICIDS << 1%\n",
    "        grid_seed = [SEED]\n",
    "    else:  # balanced\n",
    "        PREPROCS = [\n",
    "            (\"minmax\", MinMaxScaler()),\n",
    "            (\"robust\", RobustScaler()),\n",
    "            (\"quantile\", QuantileTransformer(\n",
    "                n_quantiles=min(1000, max(50, len(X_tr_df)//50)),\n",
    "                output_distribution=\"normal\", subsample=int(1e6), random_state=SEED\n",
    "            )),\n",
    "        ]\n",
    "        grid_ne   = [200, 400, 800]\n",
    "        grid_ms   = [\"auto\", 256, 512]\n",
    "        grid_cont = [0.001, 0.002, 0.005, 0.01]\n",
    "        grid_seed = [SEED, SEED+7]\n",
    "\n",
    "    prep = Pipeline([\n",
    "        (\"var0\", VarianceThreshold(0.0)),\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", PREPROCS[0][1]),                    # placeholder; wird für caching überschrieben\n",
    "        (\"cgpca\", CorrelatedGroupsPCA(0.95, 0.99) if USE_CG_PCA else \"passthrough\"),\n",
    "    ])\n",
    "\n",
    "    cached = {}  # name -> (Xtr_tf, Xva_tf, fitted_prep)\n",
    "    fit_times = []\n",
    "    best = (None, -1.0, None)  # (best_pipe, best_f1, params)\n",
    "\n",
    "    total_fits = 0\n",
    "    for _, __ in PREPROCS:\n",
    "        total_fits += len(grid_ne) * len(grid_ms) * len(grid_cont) * len(grid_seed)\n",
    "    pbar = tqdm(total=total_fits, desc=f\"IF {scenario} ({mode})\", unit=\"fit\")\n",
    "\n",
    "    for pre_name, pre in PREPROCS:\n",
    "        if pre_name not in cached:\n",
    "            t0 = time.time()\n",
    "            prep_pre = Pipeline([\n",
    "                (\"var0\", VarianceThreshold(0.0)),\n",
    "                (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", pre),\n",
    "                (\"cgpca\", CorrelatedGroupsPCA(0.95, 0.99) if USE_CG_PCA else \"passthrough\"),\n",
    "            ])\n",
    "            X_tr_tf = prep_pre.fit_transform(X_tr_df)\n",
    "            X_va_tf = prep_pre.transform(X_va_df)\n",
    "            fit_times.append(time.time() - t0)\n",
    "            cached[pre_name] = (X_tr_tf, X_va_tf, prep_pre)\n",
    "        else:\n",
    "            X_tr_tf, X_va_tf, prep_pre = cached[pre_name]\n",
    "\n",
    "        for ne in grid_ne:\n",
    "            for ms in grid_ms:\n",
    "                for cont in grid_cont:\n",
    "                    for s in grid_seed:\n",
    "                        t0 = time.time()\n",
    "                        iforest = IsolationForest(\n",
    "                            n_estimators=ne, max_samples=ms, contamination=cont,\n",
    "                            n_jobs=-1, random_state=int(s), bootstrap=False, warm_start=False\n",
    "                        )\n",
    "                        iforest.fit(X_tr_tf)  # unsupervised on benigns\n",
    "                        dt = time.time() - t0\n",
    "                        fit_times.append(dt)\n",
    "\n",
    "                        scores = -iforest.score_samples(X_va_tf)\n",
    "                        thr, f1m = best_threshold_from_scores(y_va, scores)\n",
    "\n",
    "                        if f1m > best[1]:\n",
    "                            best = ( (pre_name, ne, ms, cont, int(s), float(thr)),\n",
    "                                     float(f1m),\n",
    "                                     {\"preprocess\": pre_name, \"n_estimators\": ne,\n",
    "                                      \"max_samples\": ms, \"contamination\": cont,\n",
    "                                      \"seed\": int(s), \"threshold\": float(thr)} )\n",
    "\n",
    "                        # ETA\n",
    "                        avg = float(np.mean(fit_times))\n",
    "                        left = max(0.0, (pbar.total - pbar.n - 1) * avg)\n",
    "                        pbar.set_postfix({\"avg_s\": f\"{avg:.1f}\", \"ETA\": time.strftime(\"%H:%M:%S\", time.gmtime(left))})\n",
    "                        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    (pre_name, ne, ms, cont, s, thr) = best[0]\n",
    "    if pre_name == \"minmax\":\n",
    "        pre_obj = MinMaxScaler()\n",
    "    elif pre_name == \"robust\":\n",
    "        pre_obj = RobustScaler()\n",
    "    else:\n",
    "        pre_obj = QuantileTransformer(\n",
    "            n_quantiles=min(1000, max(50, len(X_tr_df)//50)),\n",
    "            output_distribution=\"normal\", subsample=int(1e6), random_state=SEED\n",
    "        )\n",
    "\n",
    "    final_pipe = Pipeline([\n",
    "        (\"var0\", VarianceThreshold(0.0)),\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", pre_obj),\n",
    "        (\"cgpca\", CorrelatedGroupsPCA(0.95, 0.99) if USE_CG_PCA else \"passthrough\"),\n",
    "        (\"iforest\", IsolationForest(\n",
    "            n_estimators=ne, max_samples=ms, contamination=cont,\n",
    "            n_jobs=-1, random_state=int(s), bootstrap=False, warm_start=False\n",
    "        ))\n",
    "    ])\n",
    "    t0 = time.time()\n",
    "    final_pipe.fit(X_tr_df)\n",
    "    total_time = float(np.sum(fit_times) + (time.time() - t0))\n",
    "\n",
    "    model_path = MODELS_DIR / f\"if_{scenario}.joblib\"\n",
    "    dump(final_pipe, model_path)\n",
    "\n",
    "    meta = {\n",
    "        \"scenario\": scenario,\n",
    "        \"threshold\": float(thr),\n",
    "        \"val_macro_f1_at_threshold\": best[1],\n",
    "        \"best_params\": {\n",
    "            \"preprocess\": pre_name,\n",
    "            \"n_estimators\": int(ne),\n",
    "            \"max_samples\": ms if isinstance(ms, str) else int(ms),\n",
    "            \"contamination\": float(cont),\n",
    "            \"seed\": int(s),\n",
    "        },\n",
    "        \"train_time_sec\": total_time,\n",
    "        \"use_cg_pca\": bool(USE_CG_PCA),\n",
    "        \"mode\": mode,\n",
    "    }\n",
    "    with open(MODELS_DIR / f\"if_{scenario}_meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    return {\"model_path\": str(model_path), **meta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ea82c-c3f1-4619-b123-1de1426274aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## ET-SSL — stronger encoder, cosine LR, AMP, early stopping, better aug, live ETA\n",
    "\n",
    "def train_etssl_for_scenario(\n",
    "    scenario: str,\n",
    "    epochs: int = 20,\n",
    "    batch_size: int = 2048,\n",
    "    emb_dim: int = 64,\n",
    "    proj_dim: int = 128,\n",
    "    tau: float = 0.5,\n",
    "    alpha: float = 0.95,  # slower EMA for μ_norm\n",
    "    gamma: float = 0.2,\n",
    "    weight_decay: float = 1e-5,\n",
    "    lr: float = 1e-3,\n",
    "    patience: int = 4,\n",
    "    grad_clip: float = 1.0,\n",
    ") -> Dict[str, Any]:\n",
    "    # Load splits\n",
    "    tr = load_split(\"etssl\", scenario, \"train\")\n",
    "    va = load_split(\"etssl\", scenario, \"val\")\n",
    "    te = load_split(\"etssl\", scenario, \"test\")\n",
    "    X_tr_df, _    = features_and_labels(tr)\n",
    "    X_va_df, y_va = features_and_labels(va)\n",
    "    X_te_df, _    = features_and_labels(te)\n",
    "\n",
    "    # Impute + scale on TRAIN only, then NumPy for stable sklearn interop\n",
    "    imp = SimpleImputer(strategy=\"median\").fit(X_tr_df)\n",
    "    X_tr = imp.transform(X_tr_df).astype(np.float32, copy=False)\n",
    "    X_va = imp.transform(X_va_df).astype(np.float32, copy=False)\n",
    "    X_te = imp.transform(X_te_df).astype(np.float32, copy=False)\n",
    "\n",
    "    sc  = MinMaxScaler().fit(X_tr)\n",
    "    X_tr = sc.transform(X_tr).astype(np.float32, copy=False)\n",
    "    X_va = sc.transform(X_va).astype(np.float32, copy=False)\n",
    "    X_te = sc.transform(X_te).astype(np.float32, copy=False)\n",
    "\n",
    "    d_in = X_tr.shape[1]\n",
    "\n",
    "    # Encoder with BN+Dropout for better generalization\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(self, d, emb=64, proj=128, p_drop=0.1):\n",
    "            super().__init__()\n",
    "            self.back = nn.Sequential(\n",
    "                nn.Linear(d, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(p_drop),\n",
    "                nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(p_drop),\n",
    "                nn.Linear(256, emb)\n",
    "            )\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.BatchNorm1d(emb), nn.ReLU(),\n",
    "                nn.Linear(emb, proj)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            h = self.back(x)\n",
    "            z = self.proj(h)\n",
    "            return z, h\n",
    "\n",
    "    # Stronger augmentations: noise + scaling + feature dropout\n",
    "    def augment(x, ns=0.03, drop_p=0.05):\n",
    "        noise = torch.randn_like(x) * ns\n",
    "        scale = torch.empty((x.size(0),1), device=x.device).uniform_(0.9, 1.1)\n",
    "        x2 = (x + noise) * scale\n",
    "        if drop_p > 0.0:\n",
    "            mask = (torch.rand_like(x2) > drop_p).float()\n",
    "            x2 = x2 * mask\n",
    "        return x2\n",
    "\n",
    "    # NT-Xent with temperature tau\n",
    "    def nt_xent(z1, z2, tau=0.5):\n",
    "        # L2-normalize\n",
    "        z1 = nn.functional.normalize(z1, dim=1)\n",
    "        z2 = nn.functional.normalize(z2, dim=1)\n",
    "        z  = torch.cat([z1, z2], dim=0)  # (2B, d)\n",
    "    \n",
    "        # compute similarity in float32 to avoid fp16 over/underflow\n",
    "        sim = (z.float() @ z.float().t())  # (2B, 2B), float32\n",
    "    \n",
    "        # mask self-similarity (diagonal) to -inf (safe with float32)\n",
    "        sim.fill_diagonal_(float(\"-inf\"))\n",
    "    \n",
    "        B = z1.size(0)\n",
    "        targets = torch.cat([torch.arange(B, 2*B), torch.arange(0, B)], dim=0).to(sim.device)\n",
    "    \n",
    "        # keep logits in float32 for CE stability\n",
    "        return nn.CrossEntropyLoss()(sim / tau, targets)\n",
    "\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.set_num_threads(os.cpu_count() or 8)\n",
    "\n",
    "    enc = Encoder(d_in, emb=emb_dim, proj=proj_dim).to(device)\n",
    "    opt = torch.optim.AdamW(enc.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # Cosine schedule to a small floor\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1, epochs), eta_min=lr*0.05)\n",
    "\n",
    "    dl  = DataLoader(\n",
    "        TensorDataset(torch.from_numpy(X_tr), torch.zeros(len(X_tr))),\n",
    "        batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0, pin_memory=True\n",
    "    )\n",
    "\n",
    "    scaler_amp = torch.amp.GradScaler(\"cuda\", enabled=(device==\"cuda\"))\n",
    "\n",
    "\n",
    "    # Early stopping on validation macro-F1\n",
    "    best_f1 = -1.0\n",
    "    best_state = None\n",
    "    best_meta  = None\n",
    "    no_improve = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    enc.train(); mu = None\n",
    "    for ep in trange(epochs, desc=f\"ETSSL {scenario} epochs\"):\n",
    "        tb = time.time(); steps = 0\n",
    "        pb = tqdm(total=len(dl), desc=f\"epoch {ep+1}/{epochs}\", leave=False, unit=\"step\")\n",
    "        for xb,_ in dl:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(device==\"cuda\")):\n",
    "                z1,h1 = enc(augment(xb))\n",
    "                z2,h2 = enc(augment(xb))\n",
    "                loss_c = nt_xent(z1, z2, tau=tau)\n",
    "\n",
    "                # anomaly regularization (push predicted anomalies away from μ_norm)\n",
    "                h1n = nn.functional.normalize(h1, dim=1)\n",
    "                if mu is None:\n",
    "                    mu = h1n.mean(0).detach()\n",
    "                dn = ((h1n - mu)**2).sum(1)\n",
    "                theta = torch.quantile(dn.detach(), 0.95)\n",
    "                lanom = (dn[dn > theta]).mean() if (dn > theta).any() else 0.0\n",
    "\n",
    "                loss = loss_c + gamma * (lanom if isinstance(lanom, float) else lanom)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            scaler_amp.scale(loss).backward()\n",
    "            if grad_clip is not None and grad_clip > 0:\n",
    "                scaler_amp.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(enc.parameters(), grad_clip)\n",
    "            scaler_amp.step(opt)\n",
    "            scaler_amp.update()\n",
    "\n",
    "            # EMA update for μ_norm using predicted normals\n",
    "            with torch.no_grad():\n",
    "                normals = h1n[dn <= theta]\n",
    "                if normals.size(0) > 0:\n",
    "                    mu = alpha*mu + (1-alpha)*normals.mean(0)\n",
    "\n",
    "            steps += 1\n",
    "            avg_s = (time.time()-tb) / max(1, steps)\n",
    "            left = (len(dl) - steps) * avg_s\n",
    "            pb.set_postfix({\"avg_s\": f\"{avg_s:.2f}\", \"ETA\": time.strftime(\"%H:%M:%S\", time.gmtime(left))})\n",
    "            pb.update(1)\n",
    "        pb.close()\n",
    "        sched.step()\n",
    "\n",
    "        # ---- validation at end of epoch ----\n",
    "        enc.eval()\n",
    "        with torch.no_grad():\n",
    "            def enc_np(X):\n",
    "                Z = []\n",
    "                for i in range(0, len(X), 4096):\n",
    "                    xb = torch.from_numpy(X[i:i+4096]).to(device)\n",
    "                    _, h = enc(xb)\n",
    "                    Z.append(nn.functional.normalize(h, dim=1).cpu().numpy())\n",
    "                return np.vstack(Z)\n",
    "\n",
    "            Zva = enc_np(X_va)\n",
    "\n",
    "            # centroids from validation labels (fallback if missing class)\n",
    "            mu_norm = Zva[y_va==0].mean(0) if (y_va==0).any() else Zva.mean(0)\n",
    "            if (y_va==1).any():\n",
    "                mu_anom = Zva[y_va==1].mean(0)\n",
    "            else:\n",
    "                dn_val = ((Zva - mu_norm)**2).sum(1)\n",
    "                k = max(1, int(0.02 * len(dn_val)))\n",
    "                mu_anom = Zva[np.argsort(dn_val)[-k:]].mean(0)\n",
    "\n",
    "            def scores(Z, kappa):\n",
    "                dn = ((Z - mu_norm)**2).sum(1)\n",
    "                da = ((Z - mu_anom)**2).sum(1)\n",
    "                return dn - kappa*da\n",
    "\n",
    "            # Wider kappa sweep\n",
    "            kappas = [0.0, 0.25, 0.5, 0.75, 1.0, 1.5, 2.0]\n",
    "            best_local = {\"kappa\":0.0,\"theta\":0.0,\"f1\":-1.0}\n",
    "            for k in kappas:\n",
    "                s = scores(Zva, k)\n",
    "                th, f1m = best_threshold_from_scores(y_va, s)\n",
    "                if f1m > best_local[\"f1\"]:\n",
    "                    best_local = {\"kappa\": float(k), \"theta\": float(th), \"f1\": float(f1m)}\n",
    "\n",
    "        # keep best epoch\n",
    "        if best_local[\"f1\"] > best_f1:\n",
    "            best_f1 = best_local[\"f1\"]\n",
    "            no_improve = 0\n",
    "            best_state = {k: v.detach().cpu() if isinstance(v, torch.Tensor) else v\n",
    "                          for k,v in enc.state_dict().items()}\n",
    "            best_meta = {\n",
    "                \"scenario\": scenario,\n",
    "                \"train_time_sec\": float(time.time() - t0),\n",
    "                \"emb_dim\": int(emb_dim), \"proj_dim\": int(proj_dim),\n",
    "                \"kappa\": best_local[\"kappa\"], \"theta\": best_local[\"theta\"],\n",
    "                \"mu_norm\": mu_norm.tolist(), \"mu_anom\": mu_anom.tolist(),\n",
    "                \"imputer_statistics_\": imp.statistics_.tolist(),\n",
    "                \"scaler_min_\": sc.min_.tolist() if hasattr(sc, \"min_\") else None,\n",
    "                \"scaler_scale_\": sc.scale_.tolist() if hasattr(sc, \"scale_\") else None,\n",
    "                \"best_val_macro_f1\": float(best_f1),\n",
    "                \"epochs_trained\": int(ep+1),\n",
    "                \"lr_final\": float(sched.get_last_lr()[0])\n",
    "            }\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        enc.train()\n",
    "        if no_improve >= patience:\n",
    "            print(f\"[ETSSL/{scenario}] Early stopping at epoch {ep+1} (best F1={best_f1:.4f})\")\n",
    "            break\n",
    "\n",
    "    # Save best state & meta (ensure we actually save the best)\n",
    "    if best_state is None:\n",
    "        best_state = enc.state_dict()\n",
    "        # build minimal meta if necessary\n",
    "        if best_meta is None:\n",
    "            best_meta = {\n",
    "                \"scenario\": scenario,\n",
    "                \"train_time_sec\": float(time.time() - t0),\n",
    "                \"emb_dim\": int(emb_dim), \"proj_dim\": int(proj_dim),\n",
    "                \"kappa\": 0.0, \"theta\": 0.0,\n",
    "                \"mu_norm\": np.zeros(emb_dim, dtype=float).tolist(),\n",
    "                \"mu_anom\": np.zeros(emb_dim, dtype=float).tolist(),\n",
    "                \"imputer_statistics_\": imp.statistics_.tolist(),\n",
    "                \"scaler_min_\": sc.min_.tolist() if hasattr(sc, \"min_\") else None,\n",
    "                \"scaler_scale_\": sc.scale_.tolist() if hasattr(sc, \"scale_\") else None,\n",
    "                \"best_val_macro_f1\": float(-1.0),\n",
    "                \"epochs_trained\": 0,\n",
    "                \"lr_final\": float(sched.get_last_lr()[0])\n",
    "            }\n",
    "\n",
    "    enc_path = MODELS_DIR / f\"etssl_{scenario}_encoder.pt\"\n",
    "    torch.save(best_state, enc_path)\n",
    "    with open(MODELS_DIR / f\"etssl_{scenario}_meta.json\", \"w\") as f:\n",
    "        json.dump(best_meta, f, indent=2)\n",
    "\n",
    "    return best_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845b77c-b826-48d3-9186-9db24636f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Train all models for both scenarios\n",
    "\n",
    "all_meta = {}\n",
    "\n",
    "for scenario in [\"base\", \"zeroday\"]:\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Scenario: {scenario.upper()}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    # RF\n",
    "    m_rf = train_rf(scenario)\n",
    "    all_meta[f\"rf/{scenario}\"] = m_rf\n",
    "\n",
    "    # IF\n",
    "    m_if = train_if(scenario)\n",
    "    all_meta[f\"if/{scenario}\"] = m_if\n",
    "\n",
    "    # ETSSL\n",
    "    m_et = train_etssl_for_scenario(scenario)\n",
    "    all_meta[f\"etssl/{scenario}\"] = m_et\n",
    "\n",
    "# Write a compact summary for quick inspection\n",
    "with open(MODELS_DIR / \"training_summary.json\", \"w\") as f:\n",
    "    json.dump(all_meta, f, indent=2)\n",
    "\n",
    "all_meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
