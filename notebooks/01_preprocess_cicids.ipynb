{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dccd2d90",
   "metadata": {},
   "source": [
    "# 01 — Data Preprocessing (CIC-IDS2017)\n",
    "\n",
    "This notebook cleans and prepares the **CIC-IDS2017** dataset and produces standardized splits for three approaches:\n",
    "\n",
    "- **RF (supervised)** — binary BENIGN vs. attack.  \n",
    "- **IF (unsupervised)** — trained only on BENIGN traffic, validated/tested on both classes.  \n",
    "- **ET-SSL (self-supervised)** — encoder trained on both benign and malicious (binary labels used for validation and thresholding).  \n",
    "\n",
    "### Processing steps\n",
    "1. **Column cleaning**: normalize column names, remove duplicates, standardize `Label` values (dash variants unified).  \n",
    "2. **Drop rows with missing values** (any NaN/±Inf across features).  \n",
    "3. **Drop categorical features** (keep only numeric + `Label` + binary `label`).  \n",
    "4. **Drop constant features** (manual zero-variance filter).  \n",
    "5. Create a **binary label**: `0` = BENIGN, `1` = any attack.  \n",
    "6. Keep only numeric features plus `Label` (string) and `label` (binary int).  \n",
    "7. **Train/val/test splits (70/15/15)** with class-stratification:  \n",
    "   - **RF/ET-SSL**: both benign and malicious in train/val/test.  \n",
    "   - **IF**: train restricted to BENIGN only; val/test contain both.  \n",
    "8. **Zero-day scenario**: remove `\"Bot\"`, `\"Web Attack - Brute Force\"`, and `\"Infiltration\"` from train/val; keep them in test.  \n",
    "   - RF/ET-SSL: train/val exclude these, test includes full label space.  \n",
    "   - IF: train BENIGN-only as usual; val/test include all.  \n",
    "\n",
    "### Notes\n",
    "- No normalization is applied here; MinMaxScaler is reconstructed later during training.  \n",
    "- PCA reduction (>0.95 corr → keep 0.99 var) is only applied inside RF/IF training pipelines.  \n",
    "- Splits are written to Parquet per approach/scenario.  \n",
    "\n",
    "**Input**: `./cicids2017_full_clean.parquet`  \n",
    "**Outputs**:  \n",
    "- `data/splits/<approach>/<scenario>/{train,val,test}.parquet`  \n",
    "- `data/meta/preprocess_summary.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5850583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cedric\\code\\VS\\Studium\\Hausarbeit\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Imports & configuration\n",
    "\n",
    "import os, json, time, math, warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "INPUT_PARQUET = ROOT / \"cicids2017_full_clean.parquet\"\n",
    "\n",
    "SPLITS_DIR = ROOT / \"data\" / \"splits\"\n",
    "META_DIR   = ROOT / \"data\" / \"meta\"\n",
    "for p in [SPLITS_DIR, META_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Label conventions\n",
    "BENIGN_NAME = \"BENIGN\"\n",
    "POSSIBLE_LABEL_COLS = [\"Label\", \"label\", \"labels\", \"attack\", \"Attack\", \"category\"]\n",
    "\n",
    "# Zero-day classes (must match cleaned names below)\n",
    "ZERO_DAY_ATTACKS = {\"Bot\", \"Web Attack - Brute Force\", \"Infiltration\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e83ee0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Utility helpers\n",
    "\n",
    "def find_label_column(df: pd.DataFrame) -> str:\n",
    "    for col in POSSIBLE_LABEL_COLS:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    # heuristic: last object dtype column\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    if not obj_cols:\n",
    "        raise ValueError(\"No obvious label column found; please rename your label to 'Label'.\")\n",
    "    return obj_cols[-1]\n",
    "\n",
    "def normalize_labels_column(df: pd.DataFrame, label_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Standardize strange characters and unify names.\"\"\"\n",
    "    df = df.copy()\n",
    "    # Replace weird '�' with '-'\n",
    "    if df[label_col].dtype == \"object\":\n",
    "        df[label_col] = (\n",
    "            df[label_col].astype(str)\n",
    "            .str.replace(\"�\", \"-\", regex=False)\n",
    "            .str.strip()\n",
    "        )\n",
    "    # Map a few known variants:\n",
    "    replacements = {\n",
    "        \"Web Attack – Brute Force\": \"Web Attack - Brute Force\",\n",
    "        \"Web Attack — Brute Force\": \"Web Attack - Brute Force\",\n",
    "        \"Web Attack – XSS\": \"Web Attack - XSS\",\n",
    "        \"Web Attack — XSS\": \"Web Attack - XSS\",\n",
    "        \"Web Attack – Sql Injection\": \"Web Attack - Sql Injection\",\n",
    "        \"Web Attack — Sql Injection\": \"Web Attack - Sql Injection\",\n",
    "        \"DoS Slowhttptest\": \"DoS SlowHTTPTest\",\n",
    "    }\n",
    "    df[label_col] = df[label_col].replace(replacements)\n",
    "    return df\n",
    "\n",
    "def to_binary_label(df: pd.DataFrame, label_col: str) -> pd.Series:\n",
    "    \"\"\"Return 0 for BENIGN, 1 for everything else.\"\"\"\n",
    "    return (df[label_col] != BENIGN_NAME).astype(np.int64)\n",
    "\n",
    "def drop_categorical_and_keep_label(df: pd.DataFrame, label_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Keep only numeric columns + the label column.\"\"\"\n",
    "    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c].dtype)]\n",
    "    keep = list(dict.fromkeys(num_cols + [label_col]))\n",
    "    return df[keep].copy()\n",
    "\n",
    "def remove_constant_features(df: pd.DataFrame, label_col: str) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Drop features with zero variance (constant).\"\"\"\n",
    "    df = df.copy()\n",
    "    removed = []\n",
    "    for c in list(df.columns):\n",
    "        if c == label_col: \n",
    "            continue\n",
    "        s = df[c]\n",
    "        # treat NaNs as separate — but we dropped rows with NaN already\n",
    "        if s.nunique(dropna=False) <= 1:\n",
    "            removed.append(c)\n",
    "            df.drop(columns=[c], inplace=True)\n",
    "    return df, removed\n",
    "\n",
    "def train_val_test_split_binary(\n",
    "    df: pd.DataFrame, label_col: str, train=0.70, val=0.15, test=0.15, seed=SEED\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Stratified split auf binäres Label, robust bei Miniklassen und garantiert 1D-Label.\"\"\"\n",
    "    assert abs(train + val + test - 1.0) < 1e-6\n",
    "\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep=\"first\")].copy()\n",
    "\n",
    "    y = df[label_col]\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        y = y.iloc[:, 0]\n",
    "    y = y.to_numpy().ravel()\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    indices = np.arange(len(df))\n",
    "\n",
    "    tr_idx, va_idx, te_idx = [], [], []\n",
    "    classes = np.unique(y)\n",
    "\n",
    "    for cls in classes:\n",
    "        mask = (y == cls)\n",
    "        cls_idx = indices[mask]\n",
    "        rng.shuffle(cls_idx)\n",
    "        n = len(cls_idx)\n",
    "\n",
    "        if n < 3:\n",
    "            tr_idx.extend(cls_idx)\n",
    "            continue\n",
    "\n",
    "        n_tr = max(1, int(round(train * n)))\n",
    "        n_va = max(1, int(round(val   * n)))\n",
    "        n_te = n - n_tr - n_va\n",
    "\n",
    "        if n_te < 1:\n",
    "            n_te = 1\n",
    "            if n_va > 1:\n",
    "                n_va -= 1\n",
    "            else:\n",
    "                n_tr = max(1, n_tr - 1)\n",
    "\n",
    "        tr_idx.extend(cls_idx[:n_tr])\n",
    "        va_idx.extend(cls_idx[n_tr:n_tr+n_va])\n",
    "        te_idx.extend(cls_idx[n_tr+n_va:])\n",
    "\n",
    "    tr = df.iloc[tr_idx].reset_index(drop=True)\n",
    "    va = df.iloc[va_idx].reset_index(drop=True)\n",
    "    te = df.iloc[te_idx].reset_index(drop=True)\n",
    "\n",
    "    # final shuffles\n",
    "    tr = tr.sample(frac=1.0, random_state=seed)\n",
    "    va = va.sample(frac=1.0, random_state=seed+1)\n",
    "    te = te.sample(frac=1.0, random_state=seed+2)\n",
    "    return tr, va, te\n",
    "\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_parquet(df: pd.DataFrame, path: Path):\n",
    "    ensure_dir(path.parent)\n",
    "    df = df.copy()\n",
    "    df.columns = pd.Index(df.columns).map(str)\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep=\"first\")]\n",
    "    if \"label\" in df.columns:\n",
    "        df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\").fillna(0).astype(np.int64)\n",
    "    df.to_parquet(path, index=False)\n",
    "\n",
    "\n",
    "def summary_counts(df: pd.DataFrame, label_col: str) -> Dict:\n",
    "    counts = df[label_col].value_counts().to_dict()\n",
    "    return {\"rows\": int(len(df)), \"label_counts\": {str(k): int(v) for k, v in counts.items()}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee90df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/user/project/cicids2017_full_clean.parquet ...\n",
      "Loaded shape: (2830743, 79)\n",
      "Dropping 2867 rows with any missing values ...\n",
      "Removed 8 constant feature(s).\n",
      "Final cleaned shape: (2827876, 73)\n",
      "Prep time: 4.7s\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Load dataset and run cleaning pipeline (with progress)\n",
    "\n",
    "t_start = time.time()\n",
    "if not INPUT_PARQUET.exists():\n",
    "    raise FileNotFoundError(f\"Input parquet not found: {INPUT_PARQUET}\")\n",
    "\n",
    "print(f\"Reading {INPUT_PARQUET} ...\")\n",
    "df = pd.read_parquet(INPUT_PARQUET)\n",
    "df.columns = pd.Index(df.columns).map(str).str.strip()\n",
    "dup_mask = df.columns.duplicated(keep=\"first\")\n",
    "if dup_mask.any():\n",
    "    print(f\"[warn] duplicate column names removed: {df.columns[dup_mask].tolist()}\")\n",
    "    df = df.loc[:, ~dup_mask]\n",
    "print(f\"Loaded shape: {df.shape}\")\n",
    "\n",
    "label_col = find_label_column(df)\n",
    "df = normalize_labels_column(df, label_col=label_col)\n",
    "\n",
    "# Drop NA rows (log progress in chunks)\n",
    "n_before = len(df)\n",
    "na_rows = df.isna().any(axis=1)\n",
    "n_nas = int(na_rows.sum())\n",
    "if n_nas > 0:\n",
    "    print(f\"Dropping {n_nas} rows with any missing values ...\")\n",
    "    df = df.loc[~na_rows].reset_index(drop=True)\n",
    "\n",
    "# Drop categoricals (except label)\n",
    "df = drop_categorical_and_keep_label(df, label_col=label_col)\n",
    "\n",
    "# Remove constant features\n",
    "df, removed_const = remove_constant_features(df, label_col=label_col)\n",
    "print(f\"Removed {len(removed_const)} constant feature(s).\")\n",
    "\n",
    "# Create binary column 'label'\n",
    "df = df.rename(columns={label_col: \"Label\"})\n",
    "label_col = \"Label\"\n",
    "df[\"label\"] = to_binary_label(df, label_col=\"Label\").astype(np.int64)\n",
    "\n",
    "# Quick report of attack-name normalization\n",
    "if df[\"Label\"].dtype == \"object\":\n",
    "    unique_attacks = sorted(set(df[\"Label\"].unique().tolist()))\n",
    "    # show a few\n",
    "    print(\"Unique label examples:\", unique_attacks[:10], \"... (total:\", len(unique_attacks), \")\")\n",
    "\n",
    "# Keep only numeric features + 'Label' + 'label'\n",
    "keep_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c].dtype)] + [\"Label\", \"label\"]\n",
    "df = df[keep_cols]\n",
    "\n",
    "print(f\"Final cleaned shape: {df.shape}\")\n",
    "print(f\"Prep time: {time.time() - t_start:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Split summaries:\n",
      " {\n",
      "  \"rf/base\": {\n",
      "    \"train\": {\n",
      "      \"rows\": 1979513,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 1589654,\n",
      "        \"1\": 389859\n",
      "      }\n",
      "    },\n",
      "    \"val\": {\n",
      "      \"rows\": 424181,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 340942,\n",
      "        \"1\": 83239\n",
      "      }\n",
      "    },\n",
      "    \"test\": {\n",
      "      \"rows\": 424182,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 340724,\n",
      "        \"1\": 83458\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"etssl/base\": {\n",
      "    \"train\": {\n",
      "      \"rows\": 1979513,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 1589654,\n",
      "        \"1\": 389859\n",
      "      }\n",
      "    },\n",
      "    \"val\": {\n",
      "      \"rows\": 424181,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 340942,\n",
      "        \"1\": 83239\n",
      "      }\n",
      "    },\n",
      "    \"test\": {\n",
      "      \"rows\": 424182,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 340724,\n",
      "        \"1\": 83458\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"if/base\": {\n",
      "    \"train\": {\n",
      "      \"rows\": 1589654,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 1589654\n",
      "      }\n",
      "    },\n",
      "    \"val\": {\n",
      "      \"rows\": 424181,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 340942,\n",
      "        \"1\": 83239\n",
      "      }\n",
      "    },\n",
      "    \"test\": {\n",
      "      \"rows\": 424182,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 340724,\n",
      "        \"1\": 83458\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"rf/zeroday\": {\n",
      "    \"train\": {\n",
      "      \"rows\": 1977064,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 1589964,\n",
      "        \"1\": 387100\n",
      "      }\n",
      "    },\n",
      "    \"val\": {\n",
      "      \"rows\": 423657,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 340813,\n",
      "        \"1\": 82844\n",
      "      }\n",
      "    },\n",
      "    \"test\": {\n",
      "      \"rows\": 424182,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 340724,\n",
      "        \"1\": 83458\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"etssl/zeroday\": {\n",
      "    \"train\": {\n",
      "      \"rows\": 1977064,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 1589964,\n",
      "        \"1\": 387100\n",
      "      }\n",
      "    },\n",
      "    \"val\": {\n",
      "      \"rows\": 423657,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 340813,\n",
      "        \"1\": 82844\n",
      "      }\n",
      "    },\n",
      "    \"test\": {\n",
      "      \"rows\": 424182,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 340724,\n",
      "        \"1\": 83458\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"if/zeroday\": {\n",
      "    \"train\": {\n",
      "      \"rows\": 1589654,\n",
      "      \"label_counts\": {\n",
      "        \"0\": 1589654\n",
      "      }\n",
      "    },\n",
      "    \"val\": {\n",
      "      \"rows\": 424181,\n",
      "      \"label_counts\": ...\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Build splits for both scenarios (base & zero-day) and all approaches (rf, if, etssl)\n",
    "\n",
    "def save_splits_for_rf_etssl_base(df: pd.DataFrame):\n",
    "    tr, va, te = train_val_test_split_binary(df, label_col=\"label\")\n",
    "    base_dir_rf = SPLITS_DIR / \"rf\" / \"base\"\n",
    "    base_dir_et = SPLITS_DIR / \"etssl\" / \"base\"\n",
    "    for d in [base_dir_rf, base_dir_et]:\n",
    "        ensure_dir(d)\n",
    "    for split, data in zip([\"train\", \"val\", \"test\"], [tr, va, te]):\n",
    "        write_parquet(data, base_dir_rf / f\"{split}.parquet\")\n",
    "        write_parquet(data, base_dir_et / f\"{split}.parquet\")\n",
    "    return {\n",
    "        \"rf/base\": {k: summary_counts(v, \"label\") for k, v in zip([\"train\",\"val\",\"test\"], [tr,va,te])},\n",
    "        \"etssl/base\": {k: summary_counts(v, \"label\") for k, v in zip([\"train\",\"val\",\"test\"], [tr,va,te])},\n",
    "    }\n",
    "\n",
    "def save_splits_for_if_base(df: pd.DataFrame):\n",
    "    tr_full, va_full, te_full = train_val_test_split_binary(df, label_col=\"label\")\n",
    "    tr = tr_full[tr_full[\"label\"] == 0].copy()\n",
    "    base_dir_if = SPLITS_DIR / \"if\" / \"base\"\n",
    "    ensure_dir(base_dir_if)\n",
    "    for split, data in zip([\"train\", \"val\", \"test\"], [tr, va_full, te_full]):\n",
    "        write_parquet(data, base_dir_if / f\"{split}.parquet\")\n",
    "    return {\"if/base\": {k: summary_counts(v, \"label\") for k, v in zip([\"train\",\"val\",\"test\"], [tr,va_full,te_full])}}\n",
    "\n",
    "def save_splits_zeroday(df: pd.DataFrame):\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep=\"first\")].reset_index(drop=True).copy()\n",
    "\n",
    "    Label_series = df[\"Label\"]\n",
    "    if isinstance(Label_series, pd.DataFrame):\n",
    "        Label_series = Label_series.iloc[:, 0]\n",
    "    label_text = Label_series.astype(str).to_numpy()\n",
    "    y_bin = df[\"label\"].to_numpy().reshape(-1)\n",
    "\n",
    "    is_zero_day = np.isin(label_text, list(ZERO_DAY_ATTACKS))\n",
    "\n",
    "    keep_mask = ~(np.logical_and(is_zero_day, y_bin == 1))\n",
    "    df_no_zd_mal = df.iloc[keep_mask].copy()\n",
    "\n",
    "    # RF/ETSSL\n",
    "    tr, va, _ = train_val_test_split_binary(df_no_zd_mal, label_col=\"label\")\n",
    "    \n",
    "    _, _, test_full = train_val_test_split_binary(df, label_col=\"label\")\n",
    "\n",
    "    # RF/ETSSL\n",
    "    for algo in [\"rf\", \"etssl\"]:\n",
    "        base = SPLITS_DIR / algo / \"zeroday\"\n",
    "        ensure_dir(base)\n",
    "        for split, data in zip([\"train\", \"val\", \"test\"], [tr, va, test_full]):\n",
    "            write_parquet(data, base / f\"{split}.parquet\")\n",
    "\n",
    "    # IF\n",
    "    tr_b, va_b, te_b = train_val_test_split_binary(df, label_col=\"label\")\n",
    "    tr_b = tr_b[tr_b[\"label\"] == 0].copy()\n",
    "    zif = SPLITS_DIR / \"if\" / \"zeroday\"\n",
    "    ensure_dir(zif)\n",
    "    for split, data in zip([\"train\", \"val\", \"test\"], [tr_b, va_b, te_b]):\n",
    "        write_parquet(data, zif / f\"{split}.parquet\")\n",
    "\n",
    "    return {\n",
    "        \"rf/zeroday\": {**{k: summary_counts(v, \"label\") for k, v in zip([\"train\",\"val\"], [tr,va])},\n",
    "                       \"test\": summary_counts(test_full, \"label\")},\n",
    "        \"etssl/zeroday\": {**{k: summary_counts(v, \"label\") for k, v in zip([\"train\",\"val\"], [tr,va])},\n",
    "                          \"test\": summary_counts(test_full, \"label\")},\n",
    "        \"if/zeroday\": {k: summary_counts(v, \"label\") for k, v in zip([\"train\",\"val\",\"test\"], [tr_b,va_b,te_b])},\n",
    "    }\n",
    "\n",
    "\n",
    "# Run and record metadata\n",
    "meta = {}\n",
    "meta.update(save_splits_for_rf_etssl_base(df))\n",
    "meta.update(save_splits_for_if_base(df))\n",
    "meta.update(save_splits_zeroday(df))\n",
    "\n",
    "with open(META_DIR / \"preprocess_summary.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"Done. Split summaries:\\n\", json.dumps(meta, indent=2)[:2000], \"...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
